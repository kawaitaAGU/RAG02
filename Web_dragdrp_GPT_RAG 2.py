#gpt-4o-2024-11-20
import streamlit as st
import base64
import io
import pandas as pd
from PIL import Image
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import re

# === OpenAI APIã‚­ãƒ¼ã®åˆæœŸåŒ– ========================================
if "OPENAI_API_KEY" not in st.secrets:
    st.error("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚Streamlitã®Secretsã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# === UIã‚¿ã‚¤ãƒˆãƒ«ã¨ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¬„ ===============================
st.title("ç”»åƒã‹ã‚‰å•é¡Œã‚’èª­ã¿å–ã‚Šã€RAGä»˜ãã§è‡ªå‹•è§£èª¬ï¼‹é¡ä¼¼å•é¡Œç”Ÿæˆ")

uploaded_img = st.file_uploader("å•é¡Œç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ.png, .jpgï¼‰", type=["png", "jpg", "jpeg"])

if uploaded_img:
    st.session_state.clear()

    st.image(uploaded_img, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)

    image = Image.open(uploaded_img).convert("RGB")
    buffer = io.BytesIO()
    image.save(buffer, format="PNG")
    st.session_state['b64_img'] = base64.b64encode(buffer.getvalue()).decode()
    data_uri = f"data:image/png;base64,{st.session_state['b64_img']}"

    # === GPTã«ã‚ˆã‚‹ç”»åƒå‡¦ç†ï¼ˆå•é¡Œæ–‡æŠ½å‡ºã¨RAGç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰ ===================
    with st.spinner("ç”»åƒã‚’GPTã§è§£æä¸­..."):
        extract_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": data_uri}},
                        {"type": "text", "text": "ã“ã®ç”»åƒã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å•é¡Œæ–‡ã€é¸æŠè‚¢ã€æ­£è§£ã€è§£èª¬ã€å„é¸æŠè‚¢ã®èª¬æ˜ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"}
                    ]
                }
            ],
            max_tokens=3000,
            temperature=0.0
        )
        extracted_text = extract_response.choices[0].message.content.strip()

    st.subheader("ğŸ” GPTã«ã‚ˆã‚‹å•é¡Œæƒ…å ±ã®æŠ½å‡º")
    st.markdown(extracted_text)

    # === RAGå‡¦ç†ç”¨ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ===============================
    rag_text = ""
    csv_path = Path("sample.csv")
    if not csv_path.exists():
        st.error("sample.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    try:
        with st.spinner("éå»å•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¡ä¼¼å•é¡Œã‚’æ¤œç´¢ä¸­..."):
            df = pd.read_csv(csv_path)
            if "å•é¡Œæ–‡" not in df.columns:
                st.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã« 'å•é¡Œæ–‡' åˆ—ãŒå¿…è¦ã§ã™ã€‚")
                st.stop()

            corpus = df["å•é¡Œæ–‡"].fillna("").tolist()
            vectorizer = TfidfVectorizer()
            X = vectorizer.fit_transform(corpus + [extracted_text])
            similarities = cosine_similarity(X[-1], X[:-1])[0]
            top_indices = similarities.argsort()[-3:][::-1]

            similar_questions = []
            for idx in top_indices:
                row = df.iloc[idx]
                qtext = row["å•é¡Œæ–‡"]
                choices = [str(row[c]) for c in ['a', 'b', 'c', 'd', 'e'] if c in row and pd.notna(row[c])]
                correct = str(row["è§£ç­”"]) if "è§£ç­”" in row and pd.notna(row["è§£ç­”"]) else ""
                qinfo = f"{qtext}\né¸æŠè‚¢:\n" + "\n".join([f"- {c}" for c in choices])
                if correct:
                    qinfo += f"\næ­£è§£ã¨æ€ã‚ã‚Œã‚‹é¸æŠè‚¢: {correct}"
                similar_questions.append(qinfo)
            rag_text = "\n\n".join(similar_questions)
    except Exception as e:
        st.warning(f"CSVèª­ã¿è¾¼ã¿å¤±æ•—ï¼š{e}")

    # === GPTã§è§£èª¬ãƒ»å†è©•ä¾¡ãƒ»é¡ä¼¼å•é¡Œç”Ÿæˆ =============================
    with st.spinner("GPTã«ã‚ˆã‚‹æœ€çµ‚è§£èª¬ã¨é¡ä¼¼å•é¡Œã®ç”Ÿæˆä¸­..."):
        final_prompt = (
            "ä»¥ä¸‹ã¯ç”»åƒã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå•é¡Œã¨è§£èª¬ã€é¸æŠè‚¢ã§ã‚ã‚‹ã€‚æ­£è§£ã¨ãã®æ ¹æ‹ ã‚’æ˜ç¤ºã—ã€å„é¸æŠè‚¢ã«å¯¾ã™ã‚‹è§£èª¬ã‚’è¿°ã¹ã‚ˆã€‚"
            "ãã®å¾Œã€åŒæ§˜ã®å½¢å¼ã®é¡ä¼¼å•é¡Œã‚’3å•ä½œæˆã—ã€ãã‚Œãã‚Œã«é¸æŠè‚¢ã¨æ­£è§£ã‚’ç¤ºã—ã€å„é¸æŠè‚¢ã«ã¤ã„ã¦ã‚‚èª¬æ˜ã›ã‚ˆã€‚"
            + f"\n\nå•é¡Œæƒ…å ±:\n{extracted_text}"
            + (f"\n\nä»¥ä¸‹ã¯å‚è€ƒã¨ãªã‚‹éå»å•æƒ…å ±ã§ã‚ã‚‹ï¼š\n{rag_text}" if rag_text else "")
        )

        final_response = client.chat.completions.create(
            model="gpt-4o-2024-11-20",
            messages=[{"role": "user", "content": final_prompt}],
            max_tokens=4000,
            temperature=0.3
        )
        final_output = final_response.choices[0].message.content.strip()

    # === çµæœã®è¡¨ç¤º ================================================
    st.subheader("âœ… æœ€çµ‚è§£èª¬ã¨é¡ä¼¼å•é¡Œï¼ˆGPTç”Ÿæˆï¼‰")
    st.markdown(final_output)
